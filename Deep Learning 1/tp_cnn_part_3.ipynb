{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tp_deep_learning_cnn_part_3_for_students.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPf2yufekv0f"
      },
      "source": [
        "\n",
        "# TP CNN, part 3 : super-resolution\n",
        "\n",
        "Author : Alasdair Newson\n",
        "alasdair.newson@telecom-paris.fr\n",
        " \n",
        "## Objective:\n",
        "\n",
        "We want to implement a Convolutional Neural Network (CNN) to do image super-resolution.\n",
        "\n",
        "## Image super-resolution:\n",
        "\n",
        "The super-resolution problem can be summarised as follows. We have an image as an input, which is defined over a grid $\\{0,1,\\dots, m-1\\} \\times \\{0,1,\\dots, n-1\\}$. We define a factor $\\delta$, by which we upsample the image. The output of the super-resolution is an image defined on the grid $\\{0,\\frac{1}{\\delta},\\dots, m-1\\} \\times \\{0,\\frac{1}{\\delta},\\dots, n-1\\}$.\n",
        "\n",
        "## Model\n",
        "\n",
        "In this part of the TP, you have complete freedom to create any model you want, as long as the input is an image, and the output is also an image of size $\\delta m \\times \\delta n$. You will have to choose the architecture and loss which seems reasonable to you.\n",
        "\n",
        "To help you, here is a function to upsample images in neural networks :\n",
        "\n",
        "- ```from tensorflow.keras.layers import UpSampling2D```\n",
        "\n",
        "Of course, you can use any upsampling layer you wish. \n",
        "\n",
        "## Dataset\n",
        "\n",
        "We will be using the mnist dataset for this part. This is to ensure that you can obtain good results. The input data should be the subsampled version of the mnist images, subsampled by taking one out of every $\\delta$ pixels. The output data should be the normal-resolution mnist images.\n",
        "\n",
        "__IMPORTANT NOTES:__\n",
        "- Think carefully about what the training data and labels are in this case, and create them accordingly\n",
        "- We will use ```n_max=5000``` to limit the number of datapoints (as in part 1) to go faster\n",
        "- We set $\\delta$ to 2 in this TP, because it is not too difficult to create a network that works with this factor. If you change it, it might be more difficult to create a satisfactory network.\n",
        "\n",
        "# Your task:\n",
        "You have to load the mnist data (see the first part of the TP), create the model, train it, and evaluate and display the results.\n",
        "\n",
        "We have created a function ```super_res_interpolate```, which carries out super-resolution using basic interpolation (bilinear or bicubic), with which you can compare your results visually and numerically.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5utO2_UDyKs3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5f62822-7ed5-49a7-9be4-3963e47364de"
      },
      "source": [
        "\n",
        "# # Load packages\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Input\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, UpSampling2D\n",
        "from tensorflow.keras import optimizers\n",
        "from scipy import interpolate\n",
        "print(tf.keras.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcXyExW4vgmk"
      },
      "source": [
        "This next cell is the only code you are given to carry out the TP. This function carries out a bilinear upsampling, with which you can compare your super-resolution. This function is __not__ supposed to be used by you in your network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "np3Dj3tuqEqn"
      },
      "source": [
        "# choice of the interpolation method\n",
        "interp_method = 'linear'\n",
        "# upsampling factor\n",
        "delta = 2\n",
        "# the maximum number of data to take from mnist (to go a bit faster)\n",
        "n_max = 5000\n",
        "\n",
        "# upsample by a factor of delta\n",
        "# by definition, the new grid has a step size of 1/delta\n",
        "def super_res_interpolate(imgs_in,delta,interp_method = 'linear'):\n",
        "\timgs_out = tf.image.resize( tf.constant(imgs_in),\\\n",
        "\t\t[delta*imgs_in.shape[1],delta*imgs_in.shape[2]], method='bilinear').numpy()\n",
        "\n",
        "\treturn(imgs_out)\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLGjn0zP5h0V"
      },
      "source": [
        "## Create your super-resolution network\n",
        "\n",
        "The rest is up to you ! Import the data, format it (you can use the first part of the TP as help), create your network, train it, and compare the results with ```super_res_interpolate```.\n",
        "\n",
        "Your network should be able to achieve about $80\\%$ accuracy.\n",
        "\n",
        "__Note__ you can obviously create as many cells as you like in your work."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7ELSCU3gZ0L"
      },
      "source": [
        "**1) Importing MNIST correctly (correctly formatting the data) : 1 point**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwWVMg6lkP7T"
      },
      "source": [
        "def subsample(img, delta):\n",
        "  n = img.shape[0]\n",
        "  new_n = n // delta\n",
        "  new_img = np.zeros((new_n, new_n))\n",
        "  for i in range(n):\n",
        "    for j in range(n):\n",
        "      if i%delta and j%delta:\n",
        "        new_img[i//delta][j//delta] = img[i][j]\n",
        "\n",
        "  return new_img"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1cetvuJptRa"
      },
      "source": [
        "def do_reshape(X):\n",
        "  img_rows, img_cols, nb_channels = X.shape[1], X.shape[2], 1\n",
        "  X = X.reshape(X.shape[0], img_rows, img_cols, nb_channels)\n",
        "  X = X.astype('float32')\n",
        "  X /= 255\n",
        "  return X"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-eqBm6B9gcIT",
        "outputId": "a1e86e48-fff3-4e53-ea19-1b7e58d7c6fc"
      },
      "source": [
        "from keras.datasets import mnist\n",
        "\n",
        "(X_train, _), (X_test, _) = mnist.load_data()\n",
        "X_train = X_train[0:n_max,:,:]\n",
        "X_test = X_test[0:n_max,:,:]\n",
        "\n",
        "X_train_subsampled = np.array([subsample(img, delta) for img in X_train])\n",
        "X_test_subsampled = np.array([subsample(img, delta) for img in X_test])\n",
        "\n",
        "X_train = do_reshape(X_train)\n",
        "X_test = do_reshape(X_test)\n",
        "X_train_subsampled = do_reshape(X_train_subsampled)\n",
        "X_test = do_reshape(X_test)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbeaL5g8kv7Q"
      },
      "source": [
        "**2) Creating a model which makes sense (correct input/output sizes) : 1 point**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LeunGgGjlHzl"
      },
      "source": [
        "learning_rate = 0.01\n",
        "n_epochs = 30\n",
        "batch_size = 64\n",
        "\n",
        "# number of convolutional filters to use\n",
        "nb_filters = 32\n",
        "# convolution kernel size\n",
        "kernel_size = (3, 3)\n",
        "# size of pooling area for max pooling\n",
        "pool_size = (2, 2)\n",
        "\n",
        "# --- Size of the successive layers\n",
        "n_h_1 = nb_filters\n",
        "n_h_2 = nb_filters"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbHEWpjJkwsK"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(filters=n_h_1, kernel_size=kernel_size, name='Conv1', strides=(1,1), padding='same', activation='relu'))\n",
        "model.add(Conv2D(filters=n_h_2, kernel_size=kernel_size, name='Conv2', strides=(1,1), padding='same', activation='relu'))\n",
        "model.add(UpSampling2D((2, 2)))\n",
        "model.add(Conv2D(1, (3, 3), activation='sigmoid', padding='same'))"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqczEjIXpaxD"
      },
      "source": [
        "**3) Training and achieving good results : 2 points. 1 point if the learning increases but does not reach around  80% , 2 points if the learning reaches around  80%**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5iKhE-1umdEI",
        "outputId": "f776bd6b-5433-4397-977b-79554e2fe267"
      },
      "source": [
        "model.compile(optimizer=optimizers.Adam(learning_rate=learning_rate), loss='mean_squared_error', metrics=['accuracy'])\n",
        "model.fit(X_train_subsampled, X_train, epochs=n_epochs, batch_size=batch_size)\n",
        "model.summary()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "79/79 [==============================] - 1s 8ms/step - loss: 0.0193 - accuracy: 0.8077\n",
            "Epoch 2/30\n",
            "79/79 [==============================] - 1s 8ms/step - loss: 0.0066 - accuracy: 0.8141\n",
            "Epoch 3/30\n",
            "79/79 [==============================] - 1s 8ms/step - loss: 0.0060 - accuracy: 0.8143\n",
            "Epoch 4/30\n",
            "79/79 [==============================] - 1s 8ms/step - loss: 0.0057 - accuracy: 0.8144\n",
            "Epoch 5/30\n",
            "79/79 [==============================] - 1s 8ms/step - loss: 0.0056 - accuracy: 0.8144\n",
            "Epoch 6/30\n",
            "79/79 [==============================] - 1s 8ms/step - loss: 0.0053 - accuracy: 0.8145\n",
            "Epoch 7/30\n",
            "79/79 [==============================] - 1s 8ms/step - loss: 0.0052 - accuracy: 0.8145\n",
            "Epoch 8/30\n",
            "79/79 [==============================] - 1s 8ms/step - loss: 0.0052 - accuracy: 0.8145\n",
            "Epoch 9/30\n",
            "79/79 [==============================] - 1s 8ms/step - loss: 0.0051 - accuracy: 0.8145\n",
            "Epoch 10/30\n",
            "79/79 [==============================] - 1s 8ms/step - loss: 0.0051 - accuracy: 0.8145\n",
            "Epoch 11/30\n",
            "79/79 [==============================] - 1s 8ms/step - loss: 0.0050 - accuracy: 0.8145\n",
            "Epoch 12/30\n",
            "79/79 [==============================] - 1s 8ms/step - loss: 0.0049 - accuracy: 0.8145\n",
            "Epoch 13/30\n",
            "79/79 [==============================] - 1s 8ms/step - loss: 0.0049 - accuracy: 0.8146\n",
            "Epoch 14/30\n",
            "79/79 [==============================] - 1s 8ms/step - loss: 0.0049 - accuracy: 0.8146\n",
            "Epoch 15/30\n",
            "79/79 [==============================] - 1s 8ms/step - loss: 0.0048 - accuracy: 0.8146\n",
            "Epoch 16/30\n",
            "79/79 [==============================] - 1s 9ms/step - loss: 0.0048 - accuracy: 0.8146\n",
            "Epoch 17/30\n",
            "79/79 [==============================] - 1s 8ms/step - loss: 0.0047 - accuracy: 0.8146\n",
            "Epoch 18/30\n",
            "79/79 [==============================] - 1s 8ms/step - loss: 0.0047 - accuracy: 0.8146\n",
            "Epoch 19/30\n",
            "79/79 [==============================] - 1s 8ms/step - loss: 0.0047 - accuracy: 0.8146\n",
            "Epoch 20/30\n",
            "79/79 [==============================] - 1s 8ms/step - loss: 0.0047 - accuracy: 0.8146\n",
            "Epoch 21/30\n",
            "79/79 [==============================] - 1s 9ms/step - loss: 0.0047 - accuracy: 0.8146\n",
            "Epoch 22/30\n",
            "79/79 [==============================] - 1s 9ms/step - loss: 0.0046 - accuracy: 0.8146\n",
            "Epoch 23/30\n",
            "79/79 [==============================] - 1s 9ms/step - loss: 0.0047 - accuracy: 0.8146\n",
            "Epoch 24/30\n",
            "79/79 [==============================] - 1s 8ms/step - loss: 0.0046 - accuracy: 0.8146\n",
            "Epoch 25/30\n",
            "79/79 [==============================] - 1s 9ms/step - loss: 0.0046 - accuracy: 0.8146\n",
            "Epoch 26/30\n",
            "79/79 [==============================] - 1s 8ms/step - loss: 0.0046 - accuracy: 0.8146\n",
            "Epoch 27/30\n",
            "79/79 [==============================] - 1s 9ms/step - loss: 0.0045 - accuracy: 0.8146\n",
            "Epoch 28/30\n",
            "79/79 [==============================] - 1s 9ms/step - loss: 0.0046 - accuracy: 0.8146\n",
            "Epoch 29/30\n",
            "79/79 [==============================] - 1s 8ms/step - loss: 0.0046 - accuracy: 0.8146\n",
            "Epoch 30/30\n",
            "79/79 [==============================] - 1s 8ms/step - loss: 0.0046 - accuracy: 0.8146\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " Conv1 (Conv2D)              (None, 14, 14, 32)        320       \n",
            "                                                                 \n",
            " Conv2 (Conv2D)              (None, 14, 14, 32)        9248      \n",
            "                                                                 \n",
            " up_sampling2d_2 (UpSampling  (None, 28, 28, 32)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 28, 28, 1)         289       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 9,857\n",
            "Trainable params: 9,857\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-BLeTg3qTS4",
        "outputId": "ee8a87da-0612-41f3-a63e-0d4e18bf429a"
      },
      "source": [
        "score = model.evaluate(X_train_subsampled, X_train, verbose=False)\n",
        "print('Train loss:', score[0])\n",
        "print('Train accuracy:', score[1])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 0.004689191468060017\n",
            "Train accuracy: 0.8144823908805847\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7c0WXwtQpbzX",
        "outputId": "dd342af0-85a3-48de-bf43-ac4edd48ded2"
      },
      "source": [
        "score = model.evaluate(X_test_subsampled, X_test, verbose=False)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.2969881594181061\n",
            "Test accuracy: 0.700494647026062\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4-1NECAqgOa"
      },
      "source": [
        "**4) Display a visual comparison of your network with super_res_interpolate for several examples**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngDX5VrhqhlP"
      },
      "source": [
        "images_predicted_model = model.predict(X_test_subsampled)\n",
        "images_predicted_func = super_res_interpolate(X_test_subsampled, delta)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_9Q5PzoRq1yK",
        "outputId": "76dc5003-95f0-4394-cee2-841c33442ea7"
      },
      "source": [
        "images_predicted_model.shape"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5000, 28, 28, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jNA9aFC_rN-g",
        "outputId": "f217270f-97f1-4a1b-9701-ee985bed40e9"
      },
      "source": [
        "images_predicted_func.shape"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(28, 28, 14)"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JpT6VyFHxAtp",
        "outputId": "ea2c5853-56f4-453c-ed89-c9ac95aa79f9"
      },
      "source": [
        "X_test_subsampled.shape"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5000, 14, 14)"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 761
        },
        "id": "_gtoZmbuwh8f",
        "outputId": "9b9c6c7c-0d85-4d07-b4d9-eadf4901404a"
      },
      "source": [
        "plt.imshow(X_test[1].reshape(28, 28), cmap='gray')\n",
        "plt.show()\n",
        "plt.imshow(X_test_subsampled[1].reshape(14, 14), cmap='gray')\n",
        "plt.show()\n",
        "plt.imshow(images_predicted_model[1].reshape(28, 28), cmap='gray')\n",
        "plt.show()"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PP0XHd8jwupO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nssRP8NRAQtA"
      },
      "source": [
        "# Evaluation\n",
        "\n",
        "To evaluate the work, you should rate the code for \n",
        "- 1) Importing MNIST correctly (correctly formatting the data) : 1 point\n",
        "- 2) Creating a model which makes sense (correct input/output sizes) : 1 point\n",
        "- 3) Training and achieving good results  : 2 points. 1 point if the learning increases but does not reach around $80\\%$, 2 points if the learning reaches around $80\\%$\n",
        "- 4) Display a visual comparison of your network with ```super_res_interpolate``` for several examples\n",
        "\n",
        "Total over 5 points.\n"
      ]
    }
  ]
}