{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8a2cf11-1fa9-4ae3-8f50-7e0ed848322d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "from time import time\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b48596f4-119f-4e6e-81ec-54c40d7f6276",
   "metadata": {},
   "outputs": [],
   "source": [
    "ERROR_MODE_TABLE = 'The table is not in the correct mode'\n",
    "\n",
    "READ_MODE = ['r']\n",
    "\n",
    "WRITE_MODE = ['w', 'a']\n",
    "\n",
    "SCHEMAS = {'part': ['P_PARTKEY', 'P_NAME', 'P_MFGR',\n",
    "                      'P_BRAND', 'P_TYPE', 'P_SIZE', \n",
    "                      'P_CONTAINER', 'P_RETAILPRICE', 'P_COMMENT'],\n",
    "             'supplier': ['S_SUPPKEY', 'S_NAME', 'S_ADDRESS',\n",
    "                          'S_NATIONKEY', 'S_PHONE', 'S_ACCTBAL',\n",
    "                          'S_COMMENT'],\n",
    "             'partsupp': ['PS_PARTKEY', 'PS_SUPPKEY', 'PS_AVAILQTY',\n",
    "                          'PS_SUPPLYCOST', 'PS_COMMENT'],\n",
    "             'customer': ['C_CUSTKEY', 'C_NAME', 'C_ADDRESS',\n",
    "                          'C_NATIONKEY', 'C_PHONE', 'C_ACCTBAL',\n",
    "                          'C_MKTSEGMENT', 'C_COMMENT'],\n",
    "             'orders': ['O_ORDERKEY', 'O_CUSTKEY', 'O_ORDERSTATUS',\n",
    "                        'O_TOTALPRICE', 'O_ORDERDATE', 'O_ORDERPRIORITY',\n",
    "                        'O_CLERK', 'O_SHIPPRIORITY', 'O_COMMENT'],\n",
    "             'lineitem': ['L_ORDERKEY', 'L_PARTKEY', 'L_SUPPKEY',\n",
    "                          'L_LINENUMBER', 'L_QUANTITY', 'L_EXTENDERDPRICE',\n",
    "                          'L_DISCOUNT', 'L_TAX', 'L_RETRUNFLAG',\n",
    "                          'L_LINESTATUS', 'L_SHIPDATE', 'L_COMMITDATE',\n",
    "                          'L_RECEIPTDATE', 'L_SHIPINSTRUCT', 'L_SHIPMODE',\n",
    "                          'L_COMMENT'],\n",
    "             'nation': ['N_NATIONKEY', 'N_NAME', 'N_REGIONKEY', 'N_COMMENT'],\n",
    "             'region': ['R_REGIONKEY', 'R_NAME', 'R_COMMENT']}\n",
    "\n",
    "MAX_LENGTH = 50000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a82328a-11d1-4efb-8538-a7481f59fcf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Table_COL:\n",
    "    \n",
    "    \n",
    "    #------------------------\n",
    "    #       UTILITIES\n",
    "    #------------------------\n",
    "\n",
    "    def open_table(self, mode):\n",
    "        ''' Open the table and its index in the specified mode\n",
    "\n",
    "        Inputs:\n",
    "          mode (str) : the mode to open the file ('w', 'r', 'a', 'w+')\n",
    "        '''\n",
    "\n",
    "        self.file = open(self.path_file, mode)\n",
    "        self.index = open(self.path_index, mode)\n",
    "        self.mode = mode\n",
    "        \n",
    "    def close_table(self):\n",
    "        ''' Close the table and its index and add remaning data to table\n",
    "        '''\n",
    "\n",
    "        # Before closing if the mode in write we append the data left in object\n",
    "        if self.mode in WRITE_MODE:\n",
    "            self.file.write(self.last_page_data)\n",
    "            self.last_page_data = ''\n",
    "        \n",
    "        # And close everything\n",
    "        self.file.close()\n",
    "        self.index.close()\n",
    "    \n",
    "    def write(self, data, data_id):\n",
    "        ''' Central fonction that write to table with the record/page paradigme\n",
    "\n",
    "        Inputs: \n",
    "          data (str) : data to add to the table, should be :\n",
    "                       'field|field|...|field@field|...|field@'\n",
    "          data_id (str) : the unique id of the data that will be put in the index\n",
    "                          if None, no index is added\n",
    "        ''' \n",
    "        # Verify that the table is in correct mode\n",
    "        assert self.mode in WRITE_MODE, ERROR_MODE_TABLE\n",
    "\n",
    "        len_data = len(data)        \n",
    "        # If there is more space push the data onto the page\n",
    "        if self.last_page_len + len_data < PAGE_LENGTH:\n",
    "            self.last_page_data += data\n",
    "            self.last_page_len += len_data\n",
    "\n",
    "        # Else append the data to the file\n",
    "        else:\n",
    "            self.file.write(self.last_page_data + '\\n')\n",
    "            self.last_page_data = data\n",
    "            self.last_page_len = len_data\n",
    "            self.page_num += 1\n",
    "            self.offset = 0\n",
    "        \n",
    "        if data_id:\n",
    "            # Write the index of the file\n",
    "            self.index.write(f'{data_id}|{self.page_num}|{self.offset}\\n')\n",
    "            self.offset += 1\n",
    "\n",
    "    def get_by_id(self, record_id):\n",
    "        ''' Retrun a record based on its id using the index\n",
    "\n",
    "        Input:\n",
    "          record_id (str): the unique id of the record\n",
    "        \n",
    "        Output:\n",
    "          (Table) : the record\n",
    "        '''\n",
    "\n",
    "        # Open the table and create output\n",
    "        self.open_table('r')\n",
    "        output = Table('output', '', self.fields, new_file=True)\n",
    "        output.open_table('a')\n",
    "        \n",
    "        # Transorm the id into a string as it is the format of the read\n",
    "        record_id_str = str(record_id) + '|'\n",
    "        record_id_size = len(record_id_str)\n",
    "                                      \n",
    "        # Compare the first characters of each line of the index with the id\n",
    "        for line in self.index:\n",
    "            if line[:record_id_size] == record_id_str:\n",
    "                line = line.split('|')\n",
    "                page_id, offset = int(line[1]), int(line[2])\n",
    "\n",
    "        # Read file until the right page and get the record with the offset\n",
    "        for i, line in enumerate(self.file):\n",
    "            if i == page_id:\n",
    "                result = line.strip().split('@')[offset]\n",
    "                output.write(result + '@', result.split('|')[0])\n",
    "                break\n",
    "                \n",
    "        # Close the tables\n",
    "        output.close_table()\n",
    "        self.close_table()\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def print_table(self, number_pages=1):\n",
    "        ''' Print the table beautifully to 'output_nice.txt'\n",
    "\n",
    "        Input:\n",
    "          number_pages (int): the number of the table pages that should be printed\n",
    "        '''\n",
    "        self.open_table('r')\n",
    "        self.file.readline()\n",
    "        \n",
    "        list_max_size = [len(field)+4 for field in self.fields]\n",
    "        \n",
    "        for i, page in enumerate(self.file):\n",
    "            if i < number_pages:\n",
    "                results = [record.split('|') for record in page.split('@')[:-1]]\n",
    "                tmp_lst_max_size = [max([len(record[i]) for record in results])+4 for i in range(len(self.fields))]\n",
    "                list_max_size = [max(maxi, maxi_tmp) for (maxi, maxi_tmp) in zip(list_max_size, tmp_lst_max_size)]\n",
    "            else:\n",
    "                break\n",
    "                \n",
    "        self.close_table()\n",
    "        self.open_table('r')\n",
    "        \n",
    "        with open('output_nice.txt', 'w') as nice:\n",
    "            \n",
    "            str_schema = [f'{value:^{offset}}' for value, offset in zip(self.fields, list_max_size)]\n",
    "            nice.write(' ' + '│ '.join(str_schema))\n",
    "            nice.write('\\n─' + '┼─'.join(['─'*field_len for field_len in list_max_size]) + '\\n')\n",
    "            \n",
    "            for i, page in enumerate(self.file):\n",
    "                if i < number_pages:\n",
    "                    line = [[f'{value:<{offset}}' for value, offset in zip(record.split('|'), list_max_size)]\n",
    "                                                                                for record in page.split('@')[:-1]]\n",
    "                    nice.write('\\n'.join([' ' + '│ '.join(record) for record in line]))\n",
    "                else:\n",
    "                     break\n",
    "\n",
    "        self.close_table()\n",
    "        \n",
    "    def fields2id(self, list_field_name):\n",
    "        '''Gives the position of field in the table schema\n",
    "\n",
    "        Input:\n",
    "            list_fields_name (lst [str]) : a list of fields name\n",
    "        Ouptut:\n",
    "            (lst [int]) : position of each fields\n",
    "        '''\n",
    "        list_field_id = []\n",
    "        for name in list_field_name:\n",
    "            i = 0\n",
    "            while self.fields[i] != name:\n",
    "                i += 1\n",
    "            list_field_id.append(i)\n",
    "            \n",
    "        return list_field_id\n",
    "    \n",
    "    #------------------------\n",
    "    #  AGGREGATION (SUM)\n",
    "    #------------------------\n",
    "    \n",
    "    def aggregate_sum(self, fields_to_sum, output_name='output', output_path=''):\n",
    "        \n",
    "    '''\n",
    "    Add the fields selected of each record from the table\n",
    "\n",
    "        Inputs:\n",
    "            fields_to_sum (lst [str]) : a list of field names to add (fields must be numerical)\n",
    "            output_name (str) : the name of the output file\n",
    "            output_path (str) : the path of the output file\n",
    "        \n",
    "        Output:\n",
    "          (Table) : results for each field\n",
    "    '''\n",
    "        \n",
    "        t0 = time()\n",
    "        \n",
    "        sum_fields = [0]*len(fields_to_sum)\n",
    "        fields_id = self.fields2id(fields_to_sum)\n",
    "        compt_columns = 0\n",
    "        compt_field = 0\n",
    "        \n",
    "        self.open_table('r')\n",
    "        \n",
    "        schema = self.file.readline().strip()\n",
    "\n",
    "        for page in self.file:\n",
    "            \n",
    "            columns = page.split('@')\n",
    "            len_columns = len(columns)\n",
    "\n",
    "            for i in range(len_columns):\n",
    "                \n",
    "                if columns[i] == '':\n",
    "    \n",
    "                    compt_columns += 1\n",
    "                \n",
    "                elif compt_columns in fields_id:\n",
    "                    \n",
    "                    column = columns[i].split('|')\n",
    "                    if column[-1] !='\\n':\n",
    "                        column_int = list(map(float,column))\n",
    "                    else:\n",
    "                        column_int = list(map(float,column[:-1]))\n",
    "                    \n",
    "                    sum_fields[compt_field] += sum(column_int)\n",
    "                    \n",
    "                    if (i+1)<len_columns:\n",
    "                        compt_columns += 1\n",
    "                        compt_field += 1\n",
    "                else:\n",
    "                    \n",
    "                    if (i+1)<len_columns:\n",
    "                        compt_columns += 1\n",
    "        \n",
    "        self.close_table()\n",
    "        output = Table_COL(output_name, output_path, fields_to_sum, new_file=True)\n",
    "        output.open_table('a')\n",
    "        output.write('@'.join([str(s) for s in sum_fields]) + '@', None)\n",
    "        output.close_table()\n",
    "        \n",
    "        t1 = time()\n",
    "        T = t1-t0\n",
    "        \n",
    "        print(f'The aggregation took {T:.4f}s')\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    #---------------------------------------------------\n",
    "    #  PROJECTION (WITHOUT TAKING CARE OF DUPLICATES)\n",
    "    #---------------------------------------------------\n",
    "        \n",
    "    def projection(self, fields_project, output_name='output', output_path=''):\n",
    "        \n",
    "    '''\n",
    "    Project on selected fields\n",
    "\n",
    "        Inputs:\n",
    "          field_project (lst [str]) : the fileld names to project\n",
    "          output_name (str) : the name of the output file\n",
    "          output_path (str) : the path of the output file\n",
    "\n",
    "        Output:\n",
    "          (Table) : containing all fields from the table with possible duplicates\n",
    "    '''\n",
    "        \n",
    "        t0 = time()\n",
    "        \n",
    "        output = Table_COL(output_name, output_path, fields_project, new_file=True)\n",
    "        \n",
    "        project_fields = [0]*len(fields_project)\n",
    "        fields_id = self.fields2id(fields_project)\n",
    "        compt_columns = 0\n",
    "        \n",
    "        self.open_table('r')\n",
    "        \n",
    "        schema = self.file.readline().strip()\n",
    "        \n",
    "        output.open_table('a')\n",
    "        \n",
    "        for page in self.file:\n",
    "            \n",
    "            columns = page.split('@')\n",
    "            len_columns = len(columns)\n",
    "            \n",
    "            for i in range(len_columns):\n",
    "                \n",
    "                if columns[i] == '':\n",
    "                    compt_columns += 1\n",
    "                \n",
    "                elif compt_columns in fields_id:\n",
    "                    column = columns[i].split('|')\n",
    "                    \n",
    "                    if column[-1] == '\\n':\n",
    "                         column = column[:-1]\n",
    "                    \n",
    "                    for data in column[:-1]:\n",
    "                        output.write(data+'|', None)\n",
    "                        \n",
    "                    if (i+1)<len_columns:\n",
    "                        compt_columns += 1\n",
    "                        output.write(column[-1] + '@', None)\n",
    "                else:\n",
    "                    if (i+1)<len_columns:\n",
    "                        compt_columns += 1\n",
    "                        \n",
    "        output.close_table()\n",
    "        \n",
    "        t1 = time()\n",
    "        T = t1-t0\n",
    "        \n",
    "        print(f'The projection took {T:.4f}s')\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e128c515-9248-4e8f-b1c0-4e329c3744fe",
   "metadata": {},
   "source": [
    "### Tables (for testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1184b761-0ac8-4cdb-899b-04988e41a72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'datacolumns'\n",
    "\n",
    "part_tbl = Table_COL('part', path)\n",
    "lineitem_tbl = Table_COL('lineitem', path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d95572-9f4c-4db0-927c-0688bfb61ab1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Aggregation (sum) / COLUMNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9dc31a31-c836-4c7d-ad47-e5f4ee8951c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The aggregation took 0.1864s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.Table_COL at 0x1914a823ac0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "part_tbl.aggregate_sum(['P_PARTKEY', 'P_RETAILPRICE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48cd4198-11c3-4c35-97c3-c7f0d21241b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The aggregation took 5.7043s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.Table_COL at 0x1914a842970>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lineitem_tbl.aggregate_sum(['L_QUANTITY','L_EXTENDERDPRICE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45556aee-6644-4418-953b-c85fca1e679b",
   "metadata": {},
   "source": [
    "### Projection / COLUMNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b79fbd2-701a-4bb4-8dcd-3f2acb4e8346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The projection took 0.7765s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.Table_COL at 0x1914a8423a0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "part_tbl.projection(['P_PARTKEY', 'P_NAME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9f2a526-9e02-4daf-a001-1a340aacee26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The projection took 21.5032s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.Table_COL at 0x1917bc65910>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lineitem_tbl.projection(['L_ORDERKEY','L_PARTKEY'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
